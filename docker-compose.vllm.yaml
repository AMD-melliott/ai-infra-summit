x-devices: &devices
    devices:
        - /dev/kfd
        - /dev/dri
    group_add:
        - video
x-defaults: &defaults
    security_opt:
        - seccomp:unconfined
    ipc: "host"
    shm_size: "8G"
    volumes:
      - ${CACHE_PATH}:/root/.cache/huggingface
    restart: on-failure
    environment:
      HF_HOME: /root/.cache/huggingface   
      VLLM_USE_TRITON_FLASH_ATTN: 0
      TRUST_REMOTE_CODE: "true"
      VLLM_USE_V1: 0
      VLLM_USE_AITER: 1
    env_file:
      - .env      

services:
  # Qwen3-4B Group (GPUs 0-3)
  vllm-model0:
    <<: [*defaults, *devices]  
    image: $VLLM_IMAGE
    container_name: vllm-model0
    command: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-4B --trust-remote-code --chat-template-content-format string --enable-reasoning --reasoning-parser deepseek_r1 --port 8000
    network_mode: host
    environment:
      HIP_VISIBLE_DEVICES: "0"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["qwen3-4b", "all", "default"]

  vllm-model1:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model1
    command: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-4B --trust-remote-code --chat-template-content-format string --enable-reasoning --reasoning-parser deepseek_r1 --port 8001
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "1"
    depends_on:
      vllm-model0:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["qwen3-4b", "all", "default"]

  vllm-model2:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model2
    command: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-4B --trust-remote-code --chat-template-content-format string --enable-reasoning --reasoning-parser deepseek_r1 --port 8002
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "2"
    depends_on:
      vllm-model1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["qwen3-4b", "all", "default"]

  vllm-model3:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model3
    command: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-4B --trust-remote-code --chat-template-content-format string --enable-reasoning --reasoning-parser deepseek_r1 --port 8003
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "3"
    depends_on:
      vllm-model2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["qwen3-4b", "all", "default"]

  # Qwen2.5-7B Group (GPUs 4-7)
  vllm-model4:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model4
    command: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B --trust-remote-code --chat-template-content-format string --max-model-len 16384 --port 8004
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "4"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["qwen2.5-7b", "all", "default"]

  vllm-model5:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model5
    command: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B --trust-remote-code --chat-template-content-format string --max-model-len 16384 --port 8005
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "5"
    depends_on:
      vllm-model4:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["qwen2.5-7b", "all", "default"]

  vllm-model6:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model6
    command: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B --trust-remote-code --chat-template-content-format string --max-model-len 16384 --port 8006
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "6"
    depends_on:
      vllm-model5:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8006/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["qwen2.5-7b", "all", "default"]

  vllm-model7:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model7
    command: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B --trust-remote-code --chat-template-content-format string --max-model-len 16384 --port 8007
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "7"
    depends_on:
      vllm-model6:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8007/health"]
      interval: 30s
    profiles: ["qwen2.5-7b", "all", "default"]

  # Phi-4-mini Group (GPUs 8-11)
  vllm-model8:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model8
    command: python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-4-mini-instruct --trust-remote-code --chat-template-content-format string --max-model-len 16384 --port 8008
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "8"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8008/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["phi-4-mini", "all", "default"]

  vllm-model9:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model9
    command: python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-4-mini-instruct --trust-remote-code --chat-template-content-format string --max-model-len 16384 --port 8009
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "9"
    depends_on:
      vllm-model8:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8009/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["phi-4-mini", "all", "default"]

  vllm-model10:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model10
    command: python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-4-mini-instruct --trust-remote-code --chat-template-content-format string --max-model-len 16384 --port 8010
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "10"
    depends_on:
      vllm-model9:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["phi-4-mini", "all", "default"]

  vllm-model11:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model11
    command: python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-4-mini-instruct --trust-remote-code --chat-template-content-format string --max-model-len 16384 --port 8011
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "11"
    depends_on:
      vllm-model10:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8011/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["phi-4-mini", "all", "default"]

  # Mistral-Nemo Group (GPUs 12-15)
  vllm-model12:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model12
    command: python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-Nemo-Instruct-FP8-2407 --gpu-memory-utilization 1 --max-model-len 8096 --trust-remote-code --chat-template /app/vllm/examples/tool_chat_template_mistral.jinja --tokenizer-mode mistral --port 8012
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "12"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8012/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["mistral-nemo", "all", "default"]

  vllm-model13:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model13
    command: python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-Nemo-Instruct-FP8-2407 --gpu-memory-utilization 1 --max-model-len 8096 --trust-remote-code --chat-template /app/vllm/examples/tool_chat_template_mistral.jinja --tokenizer-mode mistral --port 8013
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "13"
    depends_on:
      vllm-model12:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8013/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["mistral-nemo", "all", "default"]

  vllm-model14:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model14
    command: python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-Nemo-Instruct-FP8-2407 --gpu-memory-utilization 1 --max-model-len 8096 --trust-remote-code --chat-template /app/vllm/examples/tool_chat_template_mistral.jinja --tokenizer-mode mistral --port 8014
    network_mode: host 
    environment:
      HIP_VISIBLE_DEVICES: "14"
    depends_on:
      vllm-model13:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8014/health"] 
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["mistral-nemo", "all", "default"]

  vllm-model15:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model15
    command: python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-Nemo-Instruct-FP8-2407 --gpu-memory-utilization 1 --max-model-len 8096 --trust-remote-code --chat-template /app/vllm/examples/tool_chat_template_mistral.jinja --tokenizer-mode mistral --port 8015
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "15"
    depends_on:
      vllm-model14:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8015/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["mistral-nemo", "all", "default"]

  # BGE Embeddings Group (GPUs 16-19)
  vllm-model16:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model16
    command: python -m vllm.entrypoints.openai.api_server --model BAAI/bge-large-en-v1.5 --task embedding --port 8016
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "16"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8016/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["bge-embeddings", "all", "default"]

  vllm-model17:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model17
    command: python -m vllm.entrypoints.openai.api_server --model BAAI/bge-large-en-v1.5 --task embedding --port 8017
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "17"
    depends_on:
      vllm-model16:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8017/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["bge-embeddings", "all", "default"]

  vllm-model18:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model18
    command: python -m vllm.entrypoints.openai.api_server --model BAAI/bge-large-en-v1.5 --task embedding --port 8018
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "18"
    depends_on:
      vllm-model17:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8018/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["bge-embeddings", "all", "default"]

  vllm-model19:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model19
    command: python -m vllm.entrypoints.openai.api_server --model BAAI/bge-large-en-v1.5 --task embedding --port 8019
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "19"
    depends_on:
      vllm-model18:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8019/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["bge-embeddings", "all", "default"]

  # BGE Reranker Group (GPUs 20-23)
  vllm-model20:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model20
    command: python -m vllm.entrypoints.openai.api_server --model BAAI/bge-reranker-v2-m3 --port 8020
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "20"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8020/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["bge-reranker", "all", "default"]

  vllm-model21:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model21
    command: python -m vllm.entrypoints.openai.api_server --model BAAI/bge-reranker-v2-m3 --port 8021
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "21"
    depends_on:
      vllm-model20:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8021/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["bge-reranker", "all", "default"]

  vllm-model22:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model22
    command: python -m vllm.entrypoints.openai.api_server --model BAAI/bge-reranker-v2-m3 --port 8022
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "22"
    depends_on:
      vllm-model21:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8022/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["bge-reranker", "all", "default"]

  vllm-model23:
    <<: [*defaults, *devices]
    image: $VLLM_IMAGE
    container_name: vllm-model23
    command: python -m vllm.entrypoints.openai.api_server --model BAAI/bge-reranker-v2-m3 --port 8023
    network_mode: host 
    environment:  
      HIP_VISIBLE_DEVICES: "23"
    depends_on:
      vllm-model22:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8023/health"] 
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
    profiles: ["bge-reranker", "all", "default"]